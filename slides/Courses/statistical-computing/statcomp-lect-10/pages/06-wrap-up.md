---
level: 1
layout: section
transition: fade
---

# Wrap Up

---
level: 2
hideInToc: true
---

# Recap

- Matrix factorization, via the SVD, shows up everywhere!
- It's a great way to break up more complicated data into easier to understand parts
- The ways in which we quantify text limit (or augment) how we can learn from it
- Most classical methods are _bag of words_ models that just count instances of tokens
- Topic models function like PCA on the document term matrix
- Modern deep learning methods use vector embeddings of words to carry more information in each token


---
level: 3
---

# Final Thoughts

- [PollEv.com/klintkanopka](https://PollEv.com/klintkanopka)
