---
level: 1
layout: section
transition: fade
---

# Wrap Up

---
level: 2
hideInToc: true
---

# Recap

- Final exam
    - You can bring one $8.5 \times 11$ sheet of **handwritten** notes
    - The focus is less on memorizing specific algorithms and more on writing and understanding efficient and effective `R` code
    - Think about stuff we've hammered all semester!
- Out of sample (OOS) evaluation is always the gold standard for understanding the ability of a model to generalize
    - This becomes increasingly important as you implement more complex and flexible machine learning models
    - This is the primary way to _honestly_ communicate about model performance
- $k$-fold cross validation
    - Sometimes we don't have enough data to make fully OOS evaluation reasonable
    - Splitting your data, refitting models, and estimating performance is a good compromise
    - Cross validation is also good for tuning parameters
        - Learning rates in gradient descent
        - Temperatures in MCMC

---
level: 2
hideInToc: true
---

# Final Thoughts

- [PollEv.com/klintkanopka](https://PollEv.com/klintkanopka)

